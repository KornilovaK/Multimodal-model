{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocess atributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cats(row, all_cats):\n",
    "    d = list(ast.literal_eval(row).values())\n",
    "    all_cats.append(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = pd.read_parquet('data/raw/attributes.parquet')\n",
    "train = pd.read_parquet('data/raw/train.parquet')\n",
    "test = pd.read_parquet('data/raw/test.parquet')\n",
    "train = train[~((train.variantid1.isin(test.variantid1)) | (train.variantid2.isin(test.variantid2)))]\n",
    "attributes = attributes[(attributes.variantid.isin(train.variantid1)) | (attributes.variantid.isin(train.variantid2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06a19349924a23bd504ff41771cac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2154258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_cats = []\n",
    "attributes['vector'] = attributes.categories.progress_apply(lambda x: convert_cats(x, all_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "attributes['characters'] = attributes.characteristic_attributes_mapping.progress_apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cats_to_emb(row):\n",
    "    return np.array([cat_dict[r] for r in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = list(set(np.concatenate(attributes.vector.values)))\n",
    "cat_dict = {cat[i]: i for i in range(len(cat))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805e9eab819a46a386e03c8bf7aba9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2154258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes['vector'] = attributes.vector.progress_apply(cats_to_emb)\n",
    "attributes[['variantid', 'vector', 'characters']].to_parquet('attributes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### make description embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = text_and_bert[['variantid', 'embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"cointegrated/rubert-tiny2\")\n",
    "embeddings = model.encode(text['clean_desc'].tolist(), batch_size=32, show_progress_bar=True)\n",
    "text['desc_embs'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = text_df.rename(columns={'variantid': 'variantid1', 'embedding': 'embedding1'})\n",
    "temp1 = train_df.merge(temp1, how='inner', on='variantid1')\n",
    "\n",
    "temp2 = text_df.rename(columns={'variantid': 'variantid2', 'embedding': 'embedding2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df = resnet_df[(resnet_df.variantid.isin(train_df.variantid1) | resnet_df.variantid.isin(train_df.variantid2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df = resnet_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_df['len_emb_not_main'] = resnet_df.pic_embeddings_resnet_v1.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df['len_emb_main'] = resnet_df.main_pic_embeddings_resnet_v1.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df['pic_embeddings_resnet_v1'] = resnet_df.apply(lambda x: x.pic_embeddings_resnet_v1.sum() if x.len_emb_not_main > 0 else [0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df.main_pic_embeddings_resnet_v1 = resnet_df.main_pic_embeddings_resnet_v1.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_df['main_pic_embeddings_resnet_v1'] += resnet_df['pic_embeddings_resnet_v1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_parquet('attributes.parquet')\n",
    "resnet = pd.read_parquet('resnet.parquet')\n",
    "text_and_bert = pd.read_parquet('text_and_bert.parquet')\n",
    "\n",
    "train = pd.read_parquet('data/raw/train.parquet')\n",
    "test = pd.read_parquet('data/raw/test.parquet')\n",
    "train = train[~((train.variantid1.isin(test.variantid1)) | (train.variantid2.isin(test.variantid2)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = attributes.drop(['vector'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = text_and_bert.rename(columns={'variantid': 'variantid1', 'description': 'description1', 'name_bert_64': 'name1'})\n",
    "temp1 = train.merge(temp1, how='inner', on='variantid1')\n",
    "temp2 = text_and_bert.rename(columns={'variantid': 'variantid2', 'description': 'description2', 'name_bert_64': 'name2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp1 = resnet.rename(columns={'variantid': 'variantid1', 'main_pic_embeddings_resnet_v1': 'main1', 'pca_not_main': 'not_main1'})\n",
    "temp1 = df.merge(temp1, how='inner', on='variantid1')\n",
    "temp2 = resnet.rename(columns={'variantid': 'variantid2', 'main_pic_embeddings_resnet_v1': 'main2', 'pca_not_main': 'not_main2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = attributes.rename(columns={'variantid': 'variantid1', 'characters': 'characters1'})\n",
    "temp1 = df.merge(temp1, how='inner', on='variantid1')\n",
    "temp2 = attributes.rename(columns={'variantid': 'variantid2', 'characters': 'characters2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet('merged.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.n = len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cur_df = self.df.iloc[idx]\n",
    "        main1, main2 = torch.Tensor(cur_df.main1), torch.Tensor(cur_df.main2) #main images\n",
    "        not_main1, not_main2 = torch.Tensor(cur_df.not_main1), torch.Tensor(cur_df.not_main2) # not main images\n",
    "        attr1, attr2 = torch.tensor(cur_df.characters1), torch.tensor(cur_df.characters2) # attributes\n",
    "        description1, description2 = torch.Tensor(cur_df.description1), torch.Tensor(cur_df.description2) # descriptions\n",
    "        name1, name2 = torch.Tensor(cur_df.name1), torch.Tensor(cur_df.name2) # names\n",
    "\n",
    "        labels = torch.tensor(cur_df.target)\n",
    "        \n",
    "        return main1, main2, not_main1, not_main2, attr1, attr2, description1, description2, name1, name2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "description_size = 312\n",
    "name_size = 64\n",
    "attribute_size = 512\n",
    "embedding_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio, validation_ratio, test_ratio = 0.75, 0.15, 0.10\n",
    "\n",
    "train, test = train_test_split(df, test_size=(1 - train_ratio), stratify=df.target)\n",
    "val, test = train_test_split(test, test_size=test_ratio/(test_ratio + validation_ratio), stratify=test.target)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = MultimodalDataset(train), MultimodalDataset(val), MultimodalDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_size=128, description_size=312, name_size=64, attribute_size=512, embedding_size=256, batch_size=32):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        \n",
    "        self.main_image = nn.Sequential( nn.Linear(image_size, embedding_size), nn.ReLU() ) # [b, 128] -> [b, 256] -> [b, 256, 1]\n",
    "        self.not_main_image = nn.Sequential( nn.Linear(image_size, embedding_size), nn.ReLU() ) # [b, 128] -> [b, 256] -> [b, 256, 1]\n",
    "        self.image_embedding = nn.Sequential( nn.Conv1d(embedding_size, 128, kernel_size=2), nn.ReLU() ) # [b, 256, 2] -> [b, 128, 1]\n",
    "\n",
    "        self.name = nn.Sequential( nn.Linear(name_size, embedding_size), nn.ReLU() ) # [b, 64] -> [b, 256] -> [b, 256, 1]\n",
    "        self.description = nn.Sequential( nn.Linear(description_size, embedding_size), nn.ReLU() ) # [b, 312] -> [b, 256] -> [b, 256, 1]\n",
    "        self.text_embedding = nn.Sequential( nn.Conv1d(embedding_size, 128, kernel_size=2), nn.ReLU() ) # [b, 256, 2] -> [b, 128, 1]\n",
    "        \n",
    "        self.tabular_embedding = nn.Sequential(\n",
    "            nn.Linear(attribute_size, embedding_size), # [b, 512] -> [b, 256]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, 128), # [b, 256] -> [b, 128]\n",
    "            nn.ReLU(),\n",
    "        ) # # [b, 128] -> [b, 128, 1]\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(128, 64, kernel_size=3), # [b, 128, 3] ->  [b, 64, 1] -> [b, 64]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "            \n",
    "    def forward(self, main1, main2,\n",
    "                not_main1, not_main2,\n",
    "                attr1, attr2, description1,\n",
    "                description2, name1, name2):\n",
    "\n",
    "        main_img_emb1, main_img_emb2 = torch.unsqueeze(self.main_image(main1), 2), torch.unsqueeze(self.main_image(main2), 2)\n",
    "        img_emb1, img_emb2 = torch.unsqueeze(self.not_main_image(not_main1), 2), torch.unsqueeze(self.not_main_image(not_main2), 2)\n",
    "        \n",
    "        image_emb1 = self.image_embedding(torch.cat((main_img_emb1, img_emb1), dim=2))\n",
    "        image_emb2 = self.image_embedding(torch.cat((main_img_emb2, img_emb2), dim=2))\n",
    "\n",
    "        name_emb1, name_emb2 = torch.unsqueeze(self.name(name1), 2), torch.unsqueeze(self.name(name2), 2)\n",
    "        desc_emb1, desc_emb2 = torch.unsqueeze(self.description(description1), 2), torch.unsqueeze(self.description(description2), 2)\n",
    "        text_emb1 = self.text_embedding(torch.cat((name_emb1, desc_emb1), dim=2))\n",
    "        text_emb2 = self.text_embedding(torch.cat((name_emb2, desc_emb2), dim=2))\n",
    "\n",
    "        tab_emb1, tab_emb2 = torch.unsqueeze(self.tabular_embedding(attr1), 2), torch.unsqueeze(self.tabular_embedding(attr2), 2)\n",
    "                \n",
    "        combined1 = self.embedding(torch.cat((image_emb1, text_emb1, tab_emb1), dim=2))\n",
    "        combined2 = self.embedding(torch.cat((image_emb2, text_emb2, tab_emb2), dim=2))\n",
    "                                   \n",
    "        combined = torch.cat((torch.squeeze(combined1, 2), torch.squeeze(combined2, 2)), 1)\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contrastive_loss(distance, label, margin=1.0):\n",
    "#     loss = (1 - label) * torch.pow(distance, 2) + \\\n",
    "#            (label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)\n",
    "#     return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b5d3fae6f04856ab0f8a6f97ad268b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8122291663285738. Loss: 0.5001284942341234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2093f903ec41f5a53de13b7e8db923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8383350108600488. Loss: 0.45312686761583254\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c100bd858a48dbaf6f623f6177379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8394318343247825. Loss: 0.4511466701186657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197085b9fba461aadeec8efeb170841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8428385424944361. Loss: 0.4368260700989257\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964ac177ee07410c962dbb3faac91e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8470553378927832. Loss: 0.4341748803712436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8e17b6545b4779b2bc2fc2ab035522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8457451212061587. Loss: 0.42841810517756224\n",
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7043255c7c4c421b821a515a84564f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8531292842151008. Loss: 0.4194670418698639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002f9da2c545459d9373810b67a0ee56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.852860141901173. Loss: 0.42525227484386857\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53ded7f53dd4672bec00d474f26bd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8587729179645414. Loss: 0.4060947069344967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5775dde5c204f649944b47105f2bbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8500946700083609. Loss: 0.4201602909385962\n",
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba5bf553e0e4aeebf5e9fa6b7db3b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.864624340527097. Loss: 0.3924646028319893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7e078571ce4bbaa1dcf4f09700c777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8512774420373559. Loss: 0.4156061605920224\n",
      "Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14f616f95364c919954e04b817fe2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8720175099495444. Loss: 0.37505712212874237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12108bc218c44a4baa7b590781a04550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8618935861776558. Loss: 0.3985903608208814\n",
      "Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c16a19d6cb64abfa95e537a4b3f43e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8790522485556045. Loss: 0.3588877206657029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea357e552af43e385937361cd61401a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8647188369933233. Loss: 0.39614918848699054\n",
      "Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117b74f6c8bb4730895e8a9f2b51d5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.8840405627434961. Loss: 0.3452025564528483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7db6873f9d94ed7820948e15bcc7c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8699260380255219. Loss: 0.3942302744733534\n",
      "Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796d51096e994792bb62d36ba4b34797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PR-AUC: 0.888389055406888. Loss: 0.3340813013032531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d63a2acb6c440cb84a3bea7da93450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PR-AUC: 0.8682331856774359. Loss: 0.3964038596161535\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_auc, valid_auc = 0.0, 0.0\n",
    "    train_losses, validation_losses = 0.0, 0.0\n",
    "    \n",
    "    n1, n2 = len(train_loader), len(val_loader)\n",
    "    print(f'Epoch {epoch}')\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        main1, main2, not_main1, not_main2, \\\n",
    "        attr1, attr2, description1, description2, \\\n",
    "        name1, name2, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(main1.to(device), main2.to(device),\n",
    "                not_main1.to(device), not_main2.to(device),\n",
    "                attr1.to(device), attr2.to(device), description1.to(device),\n",
    "                description2.to(device), name1.to(device), name2.to(device))\n",
    "\n",
    "        #distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = criterion(output, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = np.argmax(F.softmax(output).cpu().detach().numpy(), axis=1)\n",
    "        preds = (y_pred > 0.5).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "        train_auc += auc(recall, precision)\n",
    "        train_losses += loss.item()\n",
    "        \n",
    "    print(f'Training PR-AUC: {train_auc / n1}. Loss: {train_losses / n1}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(val_loader)):\n",
    "            main1, main2, not_main1, not_main2, \\\n",
    "            attr1, attr2, description1, description2, \\\n",
    "            name1, name2, labels = data\n",
    "            \n",
    "            output = model(main1.to(device), main2.to(device),\n",
    "                not_main1.to(device), not_main2.to(device),\n",
    "                attr1.to(device), attr2.to(device), description1.to(device),\n",
    "                description2.to(device), name1.to(device), name2.to(device))\n",
    "            \n",
    "            loss = criterion(output, labels.to(device))\n",
    "            y_pred = np.argmax(F.softmax(output).cpu().detach().numpy(), axis=1)\n",
    "            preds = (y_pred > 0.5).astype(int)\n",
    "            precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "            valid_auc += auc(recall, precision)\n",
    "            validation_losses += loss.item()\n",
    "\n",
    "    print(f'Validation PR-AUC: {valid_auc / n2}. Loss: {validation_losses / n2}')\n",
    "\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'binary_classification.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a6f909006143b39f3aeb4c655df1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_auc = 0\n",
    "test_losses = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        main1, main2, not_main1, not_main2, \\\n",
    "        attr1, attr2, description1, description2, \\\n",
    "        name1, name2, labels = data\n",
    "        \n",
    "        output = model(main1.to(device), main2.to(device),\n",
    "            not_main1.to(device), not_main2.to(device),\n",
    "            attr1.to(device), attr2.to(device), description1.to(device),\n",
    "            description2.to(device), name1.to(device), name2.to(device))\n",
    "        \n",
    "        loss = criterion(output, labels.to(device))\n",
    "        y_pred = np.argmax(F.softmax(output).cpu().detach().numpy(), axis=1)\n",
    "        preds = (y_pred > 0.5).astype(int)\n",
    "        precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "        test_auc += auc(recall, precision)\n",
    "        test_losses += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR-AUC: 0.8684373859031199. Loss: 0.3944242146080487\n"
     ]
    }
   ],
   "source": [
    "n3 = len(test_loader)\n",
    "print(f'Test PR-AUC: {test_auc / n3}. Loss: {test_losses / n3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = resnet_df.rename(columns={'variantid': 'variantid1'})#, 'summed_embeddings': 'embedding1'})\n",
    "temp1 = train_df.merge(temp1, how='inner', on='variantid1')\n",
    "\n",
    "temp2 = resnet_df.rename(columns={'variantid': 'variantid2'})#, 'summed_embeddings': 'embedding2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = text_df.rename(columns={'variantid': 'variantid1'})#, 'clean_desc': 'clean_desc1', 'name': 'name1'})\n",
    "temp1 = df.merge(temp1, how='inner', on='variantid1')\n",
    "\n",
    "temp2 = text_df.rename(columns={'variantid': 'variantid2'})#, 'clean_desc': 'clean_desc2', 'name': 'name2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate(attr_df.vector, axis=0).reshape(2252569, 24)\n",
    "for i in range(24):\n",
    "    attr_df[str(i+1)] = a[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_df = attr_df.drop('vector', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = attr_df.rename(columns={'variantid': 'variantid1'})\n",
    "temp1 = df.merge(temp1, how='inner', on='variantid1')\n",
    "\n",
    "temp2 = attr_df.rename(columns={'variantid': 'variantid2'})\n",
    "df = temp1.merge(temp2, how='inner', on='variantid2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
      "/tmp/ipykernel_8391/2813690210.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    mask_eq =  (df[f'{i+1}_x'] == df[f'{i+1}_y']).astype(int)\n",
    "    df[f'{i+1}'] = mask_eq\n",
    "    df[f'{i+1}'][(df[f'{i+1}_x'] == 'Not reported') | (df[f'{i+1}_y'] == 'Not reported')] = 0.5\n",
    "    \n",
    "    df = df.drop([f'{i+1}_x', f'{i+1}_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 4\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TARGET_NAME = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = torch.Tensor(np.concatenate(df.summed_embeddings_x\t.to_numpy(), axis=0).reshape(len(df), 128))\n",
    "embedding2 = torch.Tensor(np.concatenate(df.summed_embeddings_y.to_numpy(), axis=0).reshape(len(df), 128))\n",
    "\n",
    "distance = F.pairwise_distance(embedding1, embedding2).numpy()\n",
    "df['image_distance'] = distance\n",
    "df = df.drop(['summed_embeddings_x', 'summed_embeddings_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = torch.Tensor(np.concatenate(df.name_bert_64_x.to_numpy(), axis=0).reshape(len(df), 64))\n",
    "name2 = torch.Tensor(np.concatenate(df.name_bert_64_y.to_numpy(), axis=0).reshape(len(df), 64))\n",
    "\n",
    "distance = F.pairwise_distance(name1, name2).numpy()\n",
    "df['name_distance'] = distance\n",
    "df = df.drop(['name_bert_64_x', 'name_bert_64_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "description1 = torch.Tensor(np.concatenate(df.description_rubert_312_x.to_numpy(), axis=0).reshape(len(df), 312))\n",
    "description2 = torch.Tensor(np.concatenate(df.description_rubert_312_y.to_numpy(), axis=0).reshape(len(df), 312))\n",
    "\n",
    "distance = F.pairwise_distance(description1, description2).numpy()\n",
    "df['description_distance'] = distance\n",
    "df = df.drop(['description_rubert_312_x', 'description_rubert_312_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('distances2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['variantid1', 'variantid2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df['target'],\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_data.drop('target', axis=1), train_data['target']\n",
    "X_test, y_test = test_data.drop('target', axis=1), test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_data.columns.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[cols] = X_train\n",
    "test_data[cols] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {'target': 'target',\n",
    "        #'drop': ['variantid1', 'variantid2']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task('binary', loss='logloss', metric='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(\n",
    "    task=task,\n",
    "    # timeout = TIMEOUT,\n",
    "    gpu_ids='0',\n",
    "    cpu_limit = N_THREADS,\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:23] Stdout logging level is INFO.\n",
      "[16:27:23] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[16:27:23] Task: binary\n",
      "\n",
      "[16:27:23] Start automl preset with listed constraints:\n",
      "[16:27:23] - time: 3600.00 seconds\n",
      "[16:27:23] - CPU: 4 cores\n",
      "[16:27:23] - memory: 16 GB\n",
      "\n",
      "[16:27:23] \u001b[1mTrain data shape: (934163, 28)\u001b[0m\n",
      "\n",
      "[16:27:30] Layer \u001b[1m1\u001b[0m train process start. Time left 3593.05 secs\n",
      "[16:27:31] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[16:28:23] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.7887975745803916\u001b[0m\n",
      "[16:28:23] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[16:28:23] Time left 3539.63 secs\n",
      "\n",
      "[16:29:26] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[16:29:26] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[16:34:49] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.917510978991514\u001b[0m\n",
      "[16:34:49] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[16:34:49] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[16:36:28] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[16:36:28] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[16:44:00] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.9180575982270777\u001b[0m\n",
      "[16:44:00] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[16:44:00] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[16:46:01] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.9108884592421317\u001b[0m\n",
      "[16:46:01] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[16:46:01] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[16:51:08] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[16:51:08] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[16:53:46] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m0.9136838081430153\u001b[0m\n",
      "[16:53:46] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[16:53:46] Time left 2016.30 secs\n",
      "\n",
      "[16:53:46] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[16:53:47] Blending: optimization starts with equal weights and score \u001b[1m0.9132803410064364\u001b[0m\n",
      "[16:53:58] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.9182668961359424\u001b[0m, weights = \u001b[1m[0.         0.35170954 0.64829046 0.         0.        ]\u001b[0m\n",
      "[16:54:10] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.9182669030720625\u001b[0m, weights = \u001b[1m[0.        0.3457091 0.6542909 0.        0.       ]\u001b[0m\n",
      "[16:54:22] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.9182669030720625\u001b[0m, weights = \u001b[1m[0.        0.3457091 0.6542909 0.        0.       ]\u001b[0m\n",
      "[16:54:22] Blending: no score update. Terminated\n",
      "\n",
      "[16:54:22] \u001b[1mAutoml preset training completed in 1618.87 seconds\u001b[0m\n",
      "\n",
      "[16:54:22] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.34571 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.65429 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) \n",
      "\n",
      "Check scores:\n",
      "OOF score: 0.9182669030720625\n",
      "TEST score: 0.9134703502080135\n",
      "CPU times: user 1h 23min 56s, sys: 5min 45s, total: 1h 29min 41s\n",
      "Wall time: 28min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_of_fold_predictions = automl.fit_predict(train_data, roles=roles, verbose=1)\n",
    "predictions = automl.predict(test_data)\n",
    "not_nan = np.any(~np.isnan(out_of_fold_predictions.data), axis=1)\n",
    "\n",
    "print('Check scores:')\n",
    "print('OOF score: {}'.format(roc_auc_score(train_labels, train_preds)))\n",
    "print('TEST score: {}'.format(roc_auc_score(test_labels, test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_preds = train_data[roles['target']].values[not_nan], out_of_fold_predictions.data[not_nan][:, 0]\n",
    "test_labels, test_preds = test_data[roles['target']].values, predictions.data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_best.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(automl, 'model_best.pkl')\n",
    "#automl=joblib.load(model.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl2 = joblib.load('model_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc(labels, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(labels, y_pred)\n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9134703502080135"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_auc(test_labels, predictions.data[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
